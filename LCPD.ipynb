{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mmahd\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PyQt5 import QtCore\n",
    "from ultralytics import YOLO\n",
    "from urllib.request import urlopen\n",
    "from keras.models import load_model\n",
    "from PyQt5.QtCore import Qt, QTimer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from object_detection.utils import label_map_util\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.preprocessing import image as keras_image\n",
    "from PyQt5.QtGui import QIcon, QPixmap, QImage, QTextCursor\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QFrame, QPushButton, QLabel, QFileDialog, QInputDialog, QLineEdit, QVBoxLayout, QWidget, QDialog, QScrollArea, QSizePolicy, QTextEdit, QMessageBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicensePlateEntryDialog(QDialog):\n",
    "    def __init__(self, parent=None):\n",
    "        super().__init__(parent)\n",
    "        self.setWindowTitle('Enter License Plate')\n",
    "        self.setGeometry(200, 200, 300, 100)\n",
    "\n",
    "        self.license_plate_entry = QLineEdit(self)\n",
    "        self.license_plate_entry.setPlaceholderText('Enter license plate')\n",
    "        self.license_plate_entry.setStyleSheet(\"background-color: white;\")\n",
    "\n",
    "        add_plate_button = QPushButton('Add License Plate', self)\n",
    "        add_plate_button.clicked.connect(self.accept)\n",
    "\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.license_plate_entry)\n",
    "        layout.addWidget(add_plate_button)\n",
    "        self.setLayout(layout)\n",
    "\n",
    "    def get_license_plate(self):\n",
    "        return self.license_plate_entry.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainWindow(QMainWindow):\n",
    "\n",
    "    ######################################################################### Settings #################################################################################\n",
    "\n",
    "    def __init__(self, app):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set Up The Main Window\n",
    "        self.setWindowTitle(\"LCPD\")\n",
    "\n",
    "        # Set Background Color\n",
    "        self.setStyleSheet(\"background-color: #6096b5;\")\n",
    "\n",
    "        # Get The Screen Resolution\n",
    "        screen_resolution = app.desktop().screenGeometry()\n",
    "        width, height = screen_resolution.width(), screen_resolution.height()\n",
    "        self.setGeometry(0, 0, width, height)\n",
    "\n",
    "        # Set Window Icon\n",
    "        icon_path = r\"D:\\University\\Project\\Media\\Icon\\Icon.png\"\n",
    "        self.setWindowIcon(QIcon(icon_path))\n",
    "\n",
    "        # Box\n",
    "        self.box = QFrame(self)\n",
    "        self.box.setGeometry(height - height, height - 130, width, width)\n",
    "        self.box.setStyleSheet(\"background-color: #9cc2d9; border: 5px solid black;\")\n",
    "\n",
    "        # Fullscreen Button\n",
    "        self.fullscreen_button = QPushButton('Fullscreen', self)\n",
    "        self.fullscreen_button.setGeometry(width - 230, height - 115, 100, 70)\n",
    "        self.fullscreen_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.fullscreen_button.clicked.connect(self.fullscreen)\n",
    "\n",
    "        # Close Button\n",
    "        self.close_button = QPushButton('Close', self)\n",
    "        self.close_button.setGeometry(width - 120, height - 115, 100, 30)\n",
    "        self.close_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.close_button.clicked.connect(self.close_application)\n",
    "\n",
    "        # Clear Button\n",
    "        self.clear_button = QPushButton('Clear', self)\n",
    "        self.clear_button.setGeometry(width - 120, height - 75, 100, 30)\n",
    "        self.clear_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.clear_button.clicked.connect(self.clear)\n",
    "\n",
    "        # Load Picture Button\n",
    "        self.load_picture_button = QPushButton('Load Picture', self)\n",
    "        self.load_picture_button.setGeometry(width - 1910, height - 115, 100, 30)\n",
    "        self.load_picture_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.load_picture_button.clicked.connect(self.load_picture)\n",
    "\n",
    "        # Load Video Button\n",
    "        self.load_video_button = QPushButton('Load Video', self)\n",
    "        self.load_video_button.setGeometry(width - 1910, height - 75, 100, 30)\n",
    "        self.load_video_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.load_video_button.clicked.connect(self.load_video)\n",
    "\n",
    "        # Detect Cars In Picture Button\n",
    "        self.detect_cars_in_picture_button = QPushButton('Detect Cars In Picture', self)\n",
    "        self.detect_cars_in_picture_button.setGeometry(width - 1800, height - 115, 150, 30)\n",
    "        self.detect_cars_in_picture_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.detect_cars_in_picture_button.clicked.connect(self.detect_cars_in_picture)\n",
    "\n",
    "        # Detect Cars In Video Button\n",
    "        self.detect_cars_in_video_button = QPushButton('Detect Cars In Video', self)\n",
    "        self.detect_cars_in_video_button.setGeometry(width - 1800, height - 75, 150, 30)\n",
    "        self.detect_cars_in_video_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.detect_cars_in_video_button.clicked.connect(self.detect_cars_in_video)\n",
    "        \n",
    "        # Real-time Detection Button\n",
    "        self.realtime_button = QPushButton('Real-time Car Detection', self)\n",
    "        self.realtime_button.setGeometry(width - 1910, height - 35, 260, 30)\n",
    "        self.realtime_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.realtime_button.clicked.connect(self.realtime_detection)\n",
    "        \n",
    "        # Detect Plate Area Button\n",
    "        self.detect_plate_area_in_picture_button = QPushButton('Detect Plate Area In Picture', self)\n",
    "        self.detect_plate_area_in_picture_button.setGeometry(width - 1640, height - 115, 200, 30)\n",
    "        self.detect_plate_area_in_picture_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.detect_plate_area_in_picture_button.clicked.connect(self.detect_plate_area_in_picture)\n",
    "        \n",
    "        # Add License Plate Button\n",
    "        self.add_plate_button = QPushButton('Add Plate Number', self)\n",
    "        self.add_plate_button.setGeometry(width - 1430, height - 115, 165, 30)\n",
    "        self.add_plate_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.add_plate_button.clicked.connect(self.show_license_plate_entry_dialog)\n",
    "        \n",
    "        # Show Plate List Button\n",
    "        self.show_plate_list_button = QPushButton('Show Plate Number List', self)\n",
    "        self.show_plate_list_button.setGeometry(width - 1430, height - 75, 165, 30)\n",
    "        self.show_plate_list_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.show_plate_list_button.clicked.connect(self.show_plate_list)\n",
    "        \n",
    "        # Detect Plate Numbers Button\n",
    "        self.detect_plate_numbers_button = QPushButton('Detect Plate Numbers', self)\n",
    "        self.detect_plate_numbers_button.setGeometry(width - 1640, height - 35, 200, 30)\n",
    "        self.detect_plate_numbers_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.detect_plate_numbers_button.clicked.connect(self.detect_plate_numbers)\n",
    "        \n",
    "        # Load Plate Picture Button\n",
    "        self.load_plate_picture_button = QPushButton('Load Plate Picture', self)\n",
    "        self.load_plate_picture_button.setGeometry(width - 1640, height - 75, 200, 30)\n",
    "        self.load_plate_picture_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.load_plate_picture_button.clicked.connect(self.load_plate_picture)\n",
    "        \n",
    "        # Detect Lane Line In The Picture Button\n",
    "        self.detect_line_in_picture_button = QPushButton('Detect Lane Line In Picture', self)\n",
    "        self.detect_line_in_picture_button.setGeometry(width - 1255, height - 115, 260, 30)\n",
    "        self.detect_line_in_picture_button.setStyleSheet(\"background-color: #f2ec35; color: black; font-weight: bold;\")\n",
    "        self.detect_line_in_picture_button.clicked.connect(self.detect_line_in_picture)\n",
    "        \n",
    "        # List to store license plates\n",
    "        self.plate_list = []\n",
    "\n",
    "        # QLabel To Show The Loaded Picture Or Video\n",
    "        self.picture_and_video_label = QLabel(self)\n",
    "        self.picture_and_video_label.setGeometry(width - width, height - height, width - 960, height - 130)\n",
    "        self.picture_and_video_label.setAlignment(Qt.AlignCenter)\n",
    "\n",
    "        # QLabel To Show The Result Picture Or Video\n",
    "        self.result_label = QLabel(self)\n",
    "        self.result_label.setGeometry(width - 960, height - height, width - 960, height - 130)\n",
    "        self.result_label.setAlignment(Qt.AlignCenter)\n",
    "\n",
    "        # QLabel to QTextEdit for the license plate list\n",
    "        self.plate_list_label = QTextEdit(self)\n",
    "        self.plate_list_label.setGeometry(width - width, height - height, width, height - 130)\n",
    "        self.plate_list_label.setAlignment(Qt.AlignTop | Qt.AlignLeft)\n",
    "        self.plate_list_label.setStyleSheet(\"background-color: white; border: 5px solid black; padding: 10px; font-weight: bold;\")\n",
    "        self.plate_list_label.hide() \n",
    "        \n",
    "        # YOLO Model\n",
    "        self.model = YOLO(r\"D:\\University\\Project\\Models\\yolo\\yolov8m.pt\")\n",
    "        \n",
    "        # Trained Model For Plate Area\n",
    "        PBTXT = r\"D:\\University\\Project\\Models\\models_for_project\\label_map.pbtxt\"\n",
    "        CONFIG = r\"D:\\University\\Project\\Models\\models_for_project\\pipeline.config\"\n",
    "        MODEL = r\"D:\\University\\Project\\Models\\models_for_project\\exported-models-V2\\my_model\\saved_model\"\n",
    "        \n",
    "        # Trained Number Detection Model \n",
    "        self.model_number = load_model(r\"D:\\University\\Project\\Models\\trained numebr detection\\LeNet_model.h5\")\n",
    "        \n",
    "        # Trained Alphabet Detection Model \n",
    "        self.model_alphabet = load_model(r\"D:\\University\\Project\\Models\\trained persian alphabet detection\\trained_model.h5\")\n",
    "        \n",
    "        # Label Mapping For Alphabet Classes\n",
    "        self.label_mapping_alphabet = {0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\", 5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\", 10: \"a\", 11: \"b\", 12: \"p\", 13: \"t\",14: \"s\", 15: \"jim\",\n",
    "                                       16: \"ch\", 17: \"ha\", 18: \"kh\", 19: \"d\", 20: \"zal\", 21: \"r\", 22: \"z\", 23: \"zh\", 24: \"sin\", 25: \"shin\", 26: \"sad\", 27: \"zad\", 28: \"ta\",\n",
    "                                       29: \"za\", 30: \"e\", 31: \"q\", 32: \"f\", 33: \"q\", 34: \"k\", 35: \"g\", 36: \"l\", 37: \"m\", 38: \"n\", 39: \"v\", 40: \"h\", 41: \"y\"}\n",
    "        \n",
    "        # Timer for video playback\n",
    "        self.timer = QTimer(self)\n",
    "\n",
    "    ######################################################################### Functions #################################################################################\n",
    "\n",
    "    def fullscreen(self):\n",
    "        if self.isFullScreen():\n",
    "            self.showNormal()\n",
    "        else:\n",
    "            self.showFullScreen()\n",
    "\n",
    "    def close_application(self):\n",
    "        self.close()\n",
    "\n",
    "    def clear(self):\n",
    "        self.picture_and_video_label.clear()\n",
    "        self.result_label.clear()\n",
    "        if hasattr(self, 'cap') and self.cap.isOpened():\n",
    "            self.cap.release()\n",
    "\n",
    "    def load_picture(self):\n",
    "        options = QFileDialog.Options()\n",
    "        options |= QFileDialog.ReadOnly\n",
    "        file_names, _ = QFileDialog.getOpenFileNames(self, \"Select Image Files\", \"\", \"Images (*.png *.jpg *.bmp *.jpeg);;All Files (*)\", options=options)\n",
    "\n",
    "        if file_names:\n",
    "            file_name = file_names[0]\n",
    "            original_image = cv2.imread(file_name)\n",
    "            if original_image is not None:\n",
    "                self.display_image(original_image)\n",
    "                self.uploaded_image = original_image\n",
    "\n",
    "    def display_image(self, image):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width, channel = image.shape\n",
    "        border_size = 5\n",
    "        bordered_image = cv2.copyMakeBorder(image, border_size, border_size, border_size, border_size, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "        q_img = QImage(bordered_image.data, width + 2 * border_size, height + 2 * border_size, bordered_image.strides[0], QImage.Format_RGB888)\n",
    "        pixmap = QPixmap.fromImage(q_img)\n",
    "        self.picture_and_video_label.setPixmap(pixmap)\n",
    "        self.picture_and_video_label.setScaledContents(True)\n",
    "\n",
    "    def load_video(self):\n",
    "        options = QFileDialog.Options()\n",
    "        options |= QFileDialog.ReadOnly\n",
    "        file_name, _ = QFileDialog.getOpenFileName(self, \"Select Video File\", \"\", \"Videos (*.mp4 *.avi *.mkv *.webm);;All Files (*)\", options=options)\n",
    "\n",
    "        if file_name:\n",
    "            self.cap = cv2.VideoCapture(file_name)\n",
    "            if self.cap.isOpened():\n",
    "                self.play_video()\n",
    "\n",
    "    def play_video(self):\n",
    "        self.timer.timeout.connect(self.display_video_frame)\n",
    "        self.timer.start(30)\n",
    "\n",
    "    def display_video_frame(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            self.timer.stop()\n",
    "            return\n",
    "        self.display_image(frame)\n",
    "\n",
    "    def detect_cars_in_picture(self):\n",
    "        if hasattr(self, 'uploaded_image'):\n",
    "            car_count = 0\n",
    "            results = self.model.predict(self.uploaded_image, verbose=False)\n",
    "            results = results[0]\n",
    "\n",
    "            for i, box in enumerate(results.boxes):\n",
    "                class_id = results.names[box.cls[0].item()]\n",
    "\n",
    "                if class_id == \"car\":\n",
    "                    cords = box.xyxy[0].tolist()\n",
    "                    cords = [round(x) for x in cords]\n",
    "                    conf = round(box.conf[0].item(), 2)\n",
    "                    x1, y1, x2, y2 = cords\n",
    "\n",
    "                    if conf >= 0.25:\n",
    "                        car_count += 1\n",
    "                        cv2.rectangle(self.uploaded_image, (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "\n",
    "            cv2.rectangle(self.uploaded_image, (10, 10), (325, 50), (0, 0, 0), -1)\n",
    "            cv2.putText(self.uploaded_image, f\"{car_count} car(s) Detected\", (15, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (125, 255, 51), 2)\n",
    "        self.display_result_image(self.uploaded_image)\n",
    "        \n",
    "    def detect_cars_in_video(self):\n",
    "        if hasattr(self, 'cap') and self.cap.isOpened():\n",
    "            self.timer.timeout.connect(self.detect_cars_and_display)\n",
    "            self.timer.start(30)\n",
    "\n",
    "    def detect_cars_and_display(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            self.timer.stop()\n",
    "            self.cap.release()\n",
    "            return\n",
    "\n",
    "        car_count = 0\n",
    "        results = self.model.predict(frame, verbose=False)\n",
    "        results = results[0]\n",
    "\n",
    "        for i, box in enumerate(results.boxes):\n",
    "            class_id = results.names[box.cls[0].item()]\n",
    "\n",
    "            if class_id == \"car\":\n",
    "                cords = box.xyxy[0].tolist()\n",
    "                cords = [round(x) for x in cords]\n",
    "                conf = round(box.conf[0].item(), 2)\n",
    "                x1, y1, x2, y2 = cords\n",
    "\n",
    "                if conf >= 0.25:\n",
    "                    car_count += 1\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "                    \n",
    "        cv2.rectangle(frame, (10, 10), (325, 50), (0, 0, 0), -1)\n",
    "        cv2.putText(frame, f\"{car_count} car(s) Detected\", (15, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (125, 255, 51), 2)\n",
    "        self.display_result_image(frame)\n",
    "        \n",
    "    def realtime_detection(self):\n",
    "        if hasattr(self, 'cap') and self.cap.isOpened():\n",
    "            self.cap.release()\n",
    "        \n",
    "        webcam_url, ok = QInputDialog.getText(self, 'Webcam URL', 'Enter the webcam URL:')\n",
    "        if not ok:\n",
    "            return\n",
    "        \n",
    "        self.cap = cv2.VideoCapture(str(webcam_url))\n",
    "\n",
    "        if not self.cap.isOpened():\n",
    "            QMessageBox.warning(self, 'Error', 'Unable to open the webcam stream.')\n",
    "            return\n",
    "\n",
    "        self.timer.timeout.connect(self.detect_cars_and_display)\n",
    "        self.timer.start(30)\n",
    "\n",
    "    def display_result_image(self, result_image):\n",
    "        result_image = cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB)\n",
    "        height, width, channel = result_image.shape\n",
    "        border_size = 5\n",
    "        bordered_image = cv2.copyMakeBorder(result_image, border_size, border_size, border_size, border_size, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "        q_img = QImage(bordered_image.data, width + 2 * border_size, height + 2 * border_size, bordered_image.strides[0], QImage.Format_RGB888)\n",
    "        pixmap = QPixmap.fromImage(q_img)\n",
    "        self.result_label.setPixmap(pixmap)\n",
    "        self.result_label.setScaledContents(True)\n",
    "        \n",
    "        \n",
    "    def detect_plate_area_in_picture(self):\n",
    "        if hasattr(self, 'uploaded_image'):\n",
    "            image = cv2.cvtColor(self.uploaded_image, cv2.COLOR_BGR2RGB)\n",
    "            img_np = np.array(image)\n",
    "            MODEL = r\"D:\\University\\Project\\Models\\models_for_project\\exported-models-V2\\my_model\\saved_model\"\n",
    "            PBTXT = r\"D:\\University\\Project\\Models\\models_for_project\\label_map.pbtxt\"\n",
    "            detector = tf.saved_model.load(MODEL)\n",
    "            category_index = label_map_util.create_category_index_from_labelmap(PBTXT, use_display_name=True)\n",
    "            detections = detector(np.expand_dims(img_np, 0))\n",
    "            num_detections = int(detections.pop('num_detections'))\n",
    "            detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "            detections['num_detections'] = num_detections\n",
    "            detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "            height, width, channel = np.shape(image)\n",
    "            image_np_with_detections = img_np.copy()\n",
    "            high_prob_indices = np.where(detections['detection_scores'] > 0.10)[0]\n",
    "            high_prob_boxes = detections['detection_boxes'][high_prob_indices]\n",
    "            high_prob_classes = detections['detection_classes'][high_prob_indices]\n",
    "            high_prob_scores = detections['detection_scores'][high_prob_indices]\n",
    "\n",
    "            if len(high_prob_indices) > 0:\n",
    "                viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                    image_np_with_detections,\n",
    "                    high_prob_boxes,\n",
    "                    high_prob_classes,\n",
    "                    high_prob_scores,\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    max_boxes_to_draw=1,\n",
    "                    min_score_thresh=0.10,\n",
    "                    agnostic_mode=False,\n",
    "                    skip_scores=False,\n",
    "                    skip_labels=False\n",
    "                )\n",
    "                \n",
    "            result_image = cv2.cvtColor(image_np_with_detections, cv2.COLOR_RGB2BGR)\n",
    "            self.display_result_image(result_image)\n",
    "            \n",
    "    def show_license_plate_entry_dialog(self):\n",
    "        dialog = LicensePlateEntryDialog(self)\n",
    "\n",
    "        if dialog.exec_():\n",
    "            plate_number = dialog.get_license_plate()\n",
    "\n",
    "            if plate_number:\n",
    "                if plate_number in self.plate_list:\n",
    "                    QMessageBox.warning(self, 'Plate Already Exists', 'The Entered Plate Number Already Exists.')\n",
    "                else:\n",
    "                    self.plate_list.append(plate_number)\n",
    "                    self.update_plate_list_label()\n",
    "\n",
    "    def update_plate_list_label(self):\n",
    "        plate_list_str = '\\n'.join(self.plate_list)\n",
    "        self.plate_list_label.setText(plate_list_str)\n",
    "\n",
    "    def show_plate_list(self):\n",
    "        self.plate_list_label.setVisible(not self.plate_list_label.isVisible())\n",
    "        \n",
    "        \n",
    "    def detect_plate_numbers(self):\n",
    "        img = cv2.imread(r\"D:\\University\\Project\\Live Proccessing\\preprocessed plate picture\\part_4.jpg\")\n",
    "        img = cv2.resize(img, (60, 60))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_array = keras_image.img_to_array(img)\n",
    "        img_array = img_array / 255.0 \n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "        predictions_alphabet = self.model_alphabet.predict(img_array)\n",
    "        predicted_class_alphabet = np.argmax(predictions_alphabet)\n",
    "        predicted_label_alphabet = self.label_mapping_alphabet.get(predicted_class_alphabet, \"Unknown\")\n",
    "\n",
    "        def preprocess_image_number(img_path):\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (32, 32))\n",
    "            img = 255 - img.astype(float)\n",
    "            return np.expand_dims(img, axis=0)\n",
    "\n",
    "        def predict_image_number(img):\n",
    "            pred = self.model_number.predict(img)\n",
    "            return pred.argmax(axis=1)[0]\n",
    "\n",
    "        folder_path = r\"D:\\University\\Project\\Live Proccessing\\preprocessed plate picture\\cropped numbers\"\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for filename in sorted(os.listdir(folder_path)):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                processed_img_number = preprocess_image_number(file_path)\n",
    "                prediction_number = predict_image_number(processed_img_number)\n",
    "                results.append(str(prediction_number))\n",
    "\n",
    "        result_str_numbers = ''.join(results)\n",
    "        result_str_combined = f\"{result_str_numbers[:2]}{predicted_label_alphabet}{result_str_numbers[2:]}\"\n",
    "        if result_str_combined in self.plate_list:\n",
    "            QMessageBox.warning(self, 'Plate Already Exists', 'Already A Member, Allowed Access.')\n",
    "        else:\n",
    "            self.plate_list.append(result_str_combined)\n",
    "            self.update_plate_list_label()\n",
    "\n",
    "    def update_plate_list_label(self):\n",
    "        plate_list_str = '\\n'.join(self.plate_list)\n",
    "        self.plate_list_label.setText(plate_list_str)\n",
    "        self.plate_list_label.moveCursor(QTextCursor.End)\n",
    "        \n",
    "        \n",
    "    def load_plate_picture(self):\n",
    "        image_path = QFileDialog.getOpenFileName(self, \"Select Plate Image\", \"\", \"Images (*.png *.jpg *.bmp *.jpeg);;All Files (*)\")[0]\n",
    "        if not image_path:\n",
    "            return None\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.imread(image_path)\n",
    "        up_crop = int(image.shape[0] * 0.1)\n",
    "        down_crop = int(image.shape[0] * 0.02)\n",
    "        left_crop = int(image.shape[1] * 0.10)\n",
    "        right_crop = int(image.shape[1] * 0)\n",
    "        image = image[up_crop:image.shape[0] - down_crop, left_crop:image.shape[1] - right_crop]\n",
    "\n",
    "        output_folder = r\"D:\\University\\Project\\Live Proccessing\\\\preprocessed plate picture\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        up_crop = int(image.shape[0] * 0)\n",
    "        down_crop = int(image.shape[0] * 0.05)\n",
    "        left_crop = int(image.shape[1] * 0)\n",
    "        right_crop = int(image.shape[1] * 0.77)\n",
    "        cropped_image_1 = image[up_crop:image.shape[0] - down_crop, left_crop:image.shape[1] - right_crop]\n",
    "        output_path_1 = os.path.join(output_folder, 'part_1.jpg')\n",
    "        cv2.imwrite(output_path_1, cropped_image_1)\n",
    "        \n",
    "        up_crop = int(image.shape[0] * 0)\n",
    "        down_crop = int(image.shape[0] * 0.05)\n",
    "        left_crop = int(image.shape[1] * 0.43)\n",
    "        right_crop = int(image.shape[1] * 0.25)\n",
    "        cropped_image_2 = image[up_crop:image.shape[0] - down_crop, left_crop:image.shape[1] - right_crop]\n",
    "        output_path_2 = os.path.join(output_folder, 'part_2.jpg')\n",
    "        cv2.imwrite(output_path_2, cropped_image_2)\n",
    "\n",
    "        up_crop = int(image.shape[0] * 0.2)\n",
    "        down_crop = int(image.shape[0] * 0)\n",
    "        left_crop = int(image.shape[1] * 0.8)\n",
    "        right_crop = int(image.shape[1] * 0.03)\n",
    "        cropped_image_3 = image[up_crop:image.shape[0] - down_crop, left_crop:image.shape[1] - right_crop]\n",
    "        output_path_3 = os.path.join(output_folder, 'part_3.jpg')\n",
    "        cv2.imwrite(output_path_3, cropped_image_3)\n",
    "        \n",
    "        up_crop = int(image.shape[0] * 0)\n",
    "        down_crop = int(image.shape[0] * 0)\n",
    "        left_crop = int(image.shape[1] * 0.20)\n",
    "        right_crop = int(image.shape[1] * 0.57)\n",
    "        cropped_image_4 = image[up_crop:image.shape[0] - down_crop, left_crop:image.shape[1] - right_crop]\n",
    "        output_path_4 = os.path.join(output_folder, 'part_4.jpg')\n",
    "        cv2.imwrite(output_path_4, cropped_image_4)\n",
    "\n",
    "        image_path = r\"D:\\University\\Project\\Live Proccessing\\preprocessed plate picture\\part_1.jpg\"\n",
    "        image = cv2.imread(image_path)\n",
    "        output_folder = r\"D:\\University\\Project\\Live Proccessing\\preprocessed plate picture\\cropped numbers\"\n",
    "        height, width = image.shape[:2]\n",
    "        left_crop = int(width * 0.55)\n",
    "        cropped_image_1 = image[:, :left_crop]\n",
    "        cropped_image_2 = image[:, left_crop:]\n",
    "        output_path_1 = os.path.join(output_folder, 'part_1_1.jpg')\n",
    "        output_path_2 = os.path.join(output_folder, 'part_1_2.jpg')\n",
    "        cv2.imwrite(output_path_1, cropped_image_1)\n",
    "        cv2.imwrite(output_path_2, cropped_image_2)\n",
    "\n",
    "        image_path = r\"D:\\University\\Project\\Live Proccessing\\preprocessed plate picture\\part_2.jpg\"\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        crop_1 = int(width * 1 / 3)\n",
    "        crop_2 = int(width * 2 / 3)\n",
    "        cropped_image_1 = image[:, :crop_1]\n",
    "        cropped_image_2 = image[:, crop_1:crop_2]\n",
    "        cropped_image_3 = image[:, crop_2:]\n",
    "        output_folder = r\"D:\\University\\Project\\Live Proccessing\\preprocessed plate picture\\cropped numbers\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_path_1 = os.path.join(output_folder, 'part_2_1.jpg')\n",
    "        output_path_2 = os.path.join(output_folder, 'part_2_2.jpg')\n",
    "        output_path_3 = os.path.join(output_folder, 'part_2_3.jpg')\n",
    "        cv2.imwrite(output_path_1, cropped_image_1)\n",
    "        cv2.imwrite(output_path_2, cropped_image_2)\n",
    "        cv2.imwrite(output_path_3, cropped_image_3)\n",
    "\n",
    "        image_path = r\"D:\\University\\Project\\Live Proccessing\\preprocessed plate picture\\part_3.jpg\"\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        left_crop = int(width * 0.5)\n",
    "        cropped_image_1 = image[:, :left_crop]\n",
    "        cropped_image_2 = image[:, left_crop:]\n",
    "        output_folder = r\"D:\\University\\Project\\Live Proccessing\\preprocessed plate picture\\cropped numbers\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_path_1 = os.path.join(output_folder, 'part_3_1.jpg')\n",
    "        output_path_2 = os.path.join(output_folder, 'part_3_2.jpg')\n",
    "\n",
    "        cv2.imwrite(output_path_1, cropped_image_1)\n",
    "        cv2.imwrite(output_path_2, cropped_image_2)\n",
    "        \n",
    "        \n",
    "    def detect_line_in_picture(self):\n",
    "        if hasattr(self, 'uploaded_image'):\n",
    "            result_image = self.lane_finding_pipeline(self.uploaded_image)\n",
    "            self.display_result_image(result_image)\n",
    "        else:\n",
    "            print(\"Please load an image first.\")\n",
    "\n",
    "    def grayscale(self, img):\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    def canny(self, img, low_threshold, high_threshold):\n",
    "        return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "    def gaussian_blur(self, img, kernel_size):\n",
    "        return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "    def region_of_interest(self, img, vertices):\n",
    "        mask = np.zeros_like(img)\n",
    "\n",
    "        if len(img.shape) > 2:\n",
    "            channel_count = img.shape[2]\n",
    "            ignore_mask_color = (255,) * channel_count\n",
    "        else:\n",
    "            ignore_mask_color = 255\n",
    "\n",
    "        cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "        masked_image = cv2.bitwise_and(img, mask)\n",
    "        return masked_image\n",
    "\n",
    "    def draw_lines(self, img, lines, color=[255, 0, 0], thickness=10):\n",
    "        for line in lines:\n",
    "            for x1, y1, x2, y2 in line:\n",
    "                cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "    def slope_lines(self, image, lines):\n",
    "        img = image.copy()\n",
    "        poly_vertices = []\n",
    "        order = [0, 1, 3, 2]\n",
    "\n",
    "        left_lines = []\n",
    "        right_lines = []\n",
    "        for line in lines:\n",
    "            for x1, y1, x2, y2 in line:\n",
    "                if x1 == x2:\n",
    "                    pass\n",
    "                else:\n",
    "                    m = (y2 - y1) / (x2 - x1)\n",
    "                    c = y1 - m * x1\n",
    "\n",
    "                    if m < 0:\n",
    "                        left_lines.append((m, c))\n",
    "                    elif m >= 0:\n",
    "                        right_lines.append((m, c))\n",
    "\n",
    "        if left_lines and right_lines:\n",
    "            left_line = np.mean(left_lines, axis=0)\n",
    "            right_line = np.mean(right_lines, axis=0)\n",
    "\n",
    "            for slope, intercept in [left_line, right_line]:\n",
    "                rows, cols = image.shape[:2]\n",
    "                y1 = int(rows)\n",
    "                y2 = int(rows * 0.6)\n",
    "                x1 = int((y1 - intercept) / slope)\n",
    "                x2 = int((y2 - intercept) / slope)\n",
    "                poly_vertices.append((x1, y1))\n",
    "                poly_vertices.append((x2, y2))\n",
    "                self.draw_lines(img, np.array([[[x1, y1, x2, y2]]]))\n",
    "\n",
    "            poly_vertices = [poly_vertices[i] for i in order]\n",
    "            cv2.fillPoly(img, pts=np.array([poly_vertices], 'int32'), color=(0, 255, 0))\n",
    "            return cv2.addWeighted(image, 0.7, img, 0.4, 0.)\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    def hough_lines(self, img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "        lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len,\n",
    "                                maxLineGap=max_line_gap)\n",
    "        if lines is not None:\n",
    "            line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "            line_img = self.slope_lines(line_img, lines)\n",
    "            return line_img\n",
    "        else:\n",
    "            print(\"No lines detected.\")\n",
    "            return img\n",
    "\n",
    "    def weighted_img(self, img, initial_img, α=0.1, β=1., γ=0.):\n",
    "        try:\n",
    "            lines_edges = cv2.addWeighted(initial_img, α, img, β, γ)\n",
    "            return lines_edges\n",
    "        except Exception as e:\n",
    "            print(f\"Error in weighted_img: {e}\")\n",
    "            return initial_img\n",
    "\n",
    "\n",
    "    def get_vertices(self, image):\n",
    "        rows, cols = image.shape[:2]\n",
    "        bottom_left = [cols * 0.15, rows]\n",
    "        top_left = [cols * 0.45, rows * 0.6]\n",
    "        bottom_right = [cols * 0.95, rows]\n",
    "        top_right = [cols * 0.55, rows * 0.6]\n",
    "\n",
    "        ver = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
    "        return ver\n",
    "\n",
    "    def lane_finding_pipeline(self, image):\n",
    "        gray_img = self.grayscale(image)\n",
    "        smoothed_img = self.gaussian_blur(img=gray_img, kernel_size=5)\n",
    "        canny_img = self.canny(img=smoothed_img, low_threshold=180, high_threshold=240)\n",
    "        masked_img = self.region_of_interest(img=canny_img, vertices=self.get_vertices(image))\n",
    "        houghed_lines = self.hough_lines(img=masked_img, rho=1, theta=np.pi / 180, threshold=20,\n",
    "                                         min_line_len=20, max_line_gap=180)\n",
    "        output = self.weighted_img(img=houghed_lines, initial_img=image, α=0.8, β=1., γ=0.)\n",
    "        return output\n",
    "    \n",
    "\n",
    "def main():\n",
    "    app = QApplication(sys.argv)\n",
    "    window = MainWindow(app)\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mmahd\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mmahd\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
